{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an attempt to create a recommendation engine with this dataset. Our Naive assumption is that a person's taste in film does not evolve with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "%pylab inline\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/movie_metadata.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cleaning the data is not the focus of this notebook, I'll just dump it all in one cell. That way we can skip over to the nice parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_actors = set(df.actor_1_name.unique())\n",
    "second_actors = set(df.actor_2_name.unique())\n",
    "third_actors = set(df.actor_3_name.unique())\n",
    "print('Those only in first name', len(first_actors - second_actors - third_actors))\n",
    "print('Those only in second name', len(second_actors - first_actors - third_actors))\n",
    "print('Those only in third name', len(third_actors - first_actors - second_actors))\n",
    "# ----is it color or not\n",
    "df.color = df.color.map({'Color': 1, ' Black and White':0})\n",
    "# ---- Genres as on-off flags instead of strings\n",
    "unique_genre_labels = set()\n",
    "for genre_flags in df.genres.str.split('|').values:\n",
    "    unique_genre_labels = unique_genre_labels.union(set(genre_flags))\n",
    "# If Genres contains, 1 otherwise, 0\n",
    "for label in unique_genre_labels:\n",
    "    df['Genre='+label] = df.genres.str.contains(label).astype(int)\n",
    "df = df.drop('genres', axis=1)\n",
    "\n",
    "# Titles are supposed to be unique right?\n",
    "if len(df.drop_duplicates(subset=['movie_title',\n",
    "                                  'title_year',\n",
    "                                  'movie_imdb_link'])) < len(df):\n",
    "    print('Duplicate Titles Exist')\n",
    "    # Let's see these duplicates.\n",
    "    duplicates = df[df.movie_title.map(df.movie_title.value_counts() > 1)]\n",
    "    duplicates.sort('movie_title')[['movie_title', 'title_year']]\n",
    "    # Looks like there are duplicates after all. Let's drop those.\n",
    "    df = df.drop_duplicates(subset=['movie_title', 'title_year', 'movie_imdb_link'])\n",
    "    # df.info()\n",
    "counts = df.language.value_counts()\n",
    "df.language = df.language.map(counts)\n",
    "#df.language\n",
    "count = df.country.value_counts()\n",
    "df.country = df.country.map(count)\n",
    "#df.country\n",
    "counts = df.content_rating.value_counts()\n",
    "df.content_rating = df.content_rating.map(counts)\n",
    "#df.content_rating\n",
    "#df.plot_keywords.head()\n",
    "unique_words = set()\n",
    "for wordlist in df.plot_keywords.str.split('|').values:\n",
    "    if wordlist is not np.nan:\n",
    "        unique_words = unique_words.union(set(wordlist))\n",
    "plot_wordbag = list(unique_words)\n",
    "for word in plot_wordbag:\n",
    "    df['plot_has_' + word.replace(' ', '-')] = df.plot_keywords.str.contains(word).astype(float)\n",
    "df = df.drop('plot_keywords', axis=1)\n",
    "# Is anything left to be done other than imputing?\n",
    "print(df.select_dtypes(include=['O']).columns)\n",
    "# We replace director name with counts of movies they've done\n",
    "df.director_name = df.director_name.map(df.director_name.value_counts())\n",
    "# We replace actor names with the number of movies they appear in.\n",
    "counts = pd.concat([df.actor_1_name, df.actor_2_name, df.actor_3_name]).value_counts()\n",
    "#counts.head()\n",
    "df.actor_1_name = df.actor_1_name.map(counts)\n",
    "df.actor_2_name = df.actor_2_name.map(counts)\n",
    "df.actor_3_name = df.actor_3_name.map(counts)\n",
    "# I have no clue what to do with the title. I'll keep it for now in order to search by name\n",
    "df = df.drop(['movie_imdb_link'], axis=1)\n",
    "# Let's check if anything is left as object\n",
    "df.select_dtypes(include=['O']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the data is clean enough. Recommend already!\n",
    "It's filled with holes though. Pun intended. :D\n",
    "\n",
    "I wanted to try out some fancy imputation (there's a package by that name too) so here goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hold your horses, we still need to fill those missing values.\n",
    "new_style = {'grid': False}\n",
    "matplotlib.rc('axes', **new_style)\n",
    "plt.matshow(~df.isnull())\n",
    "plt.title('Missing values in the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's get those rows which are mostly incomplete. I suspect this was because of our\n",
    "# new features being created from old ones which were null.\n",
    "nullcount = df.isnull().sum(axis=1)\n",
    "# Let's just keep those who have less than a hundred missing values\n",
    "ndf = df.dropna(thresh=100)\n",
    "print(ndf.shape, df.shape)\n",
    "# Let's see those nulls again\n",
    "plt.matshow(~ndf.isnull())\n",
    "plt.title('Missing values in the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll treat fillna as a regression / classification problem here.\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "def reg_class_fill(df, column, classifier):\n",
    "    \"\"\"Treat missing values as a classification / regresion problem\"\"\"\n",
    "    ndf = df.dropna(subset=[col for col in df.columns if col != column])\n",
    "    nullmask = ndf[column].isnull()\n",
    "    train, test  = ndf[~nullmask], ndf[nullmask]\n",
    "    train_x, train_y = train.drop(column, axis=1), train[column]\n",
    "    classifier.fit(train_x, train_y)\n",
    "    if len(test) > 0:\n",
    "        test_x, test_y = test.drop(column, axis=1), test[column]\n",
    "        values = classifier.predict(test_x)\n",
    "        test_y = values\n",
    "        new_x, new_y = pd.concat([train_x, test_x]), pd.concat([train_y, test_y])\n",
    "        newdf = new_x[column] = new_y\n",
    "        return newdf\n",
    "    else:\n",
    "        return ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r, c = KNeighborsRegressor, KNeighborsClassifier  # Regress or classify\n",
    "title_encoder = LabelEncoder()\n",
    "title_encoder.fit(ndf.movie_title)\n",
    "ndf.movie_title = title_encoder.transform(ndf.movie_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(ndf[ndf.columns[:25]].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since our imputation will impact other imputations, we specify an order\n",
    "# Typically we should do this independently and then combine the results, but meh for now\n",
    "impute_order = [('director_name', c), ('title_year', c),\n",
    "                ('actor_1_name', c), ('actor_2_name', c), ('actor_3_name', c),\n",
    "                ('gross', r), ('budget', r), ('aspect_ratio', r),\n",
    "                ('content_rating', r), ('num_critic_for_reviews', r)]\n",
    "for col, classifier in impute_order:\n",
    "    ndf = reg_class_fill(ndf, col, classifier())\n",
    "    print(col, 'Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Again we check for what else needs to be imputed.\n",
    "ndf[ndf.columns[:25]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Did we get everything?\n",
    "ndf.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YAY! We did indeed get everything, though it may not have been very good.\n",
    "# Now we redo the movie title transformation for our searches.\n",
    "titles = title_encoder.inverse_transform(ndf.movie_title)\n",
    "#titles = [i.lower().strip() for i in titles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And we are ready to recommend stuff to you love :D\n",
    "We build a simple KD tree recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Give us 5 movies that you liked\n",
    "def get_movies(names):\n",
    "    movies = []\n",
    "    for name in names:\n",
    "        found = [i for i in titles if name.lower() in i.lower()]\n",
    "        if len(found) > 0:\n",
    "            movies.append(found[0])\n",
    "            print(name, ': ', found, 'added', movies[-1], 'to movies')\n",
    "        else:\n",
    "            print(name, ': ', found)\n",
    "    print('-'*10)\n",
    "    print(movies)\n",
    "    moviecodes = title_encoder.transform(movies)\n",
    "    return moviecodes, movies\n",
    "names = ['fight club', 'gump', # This one is Forrest Gump\n",
    "                 'usual suspects', 'silence of the lambs']\n",
    "moviecodes, movies = get_movies(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = ndf.drop('movie_title', axis=1)\n",
    "data = MinMaxScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We assume KNN's assumptions as valid and proceede to compute a distance_matrix\n",
    "from sklearn.neighbors import KDTree\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = KDTree(data, leaf_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend(movies, tree, titles, data):\n",
    "    \"\"\"\n",
    "    It is assumed that the movies are in order of decreasing like-able-ness\n",
    "    Recommend movies on the basis of the KDTree generated.\n",
    "    Return them in order of increasing distance form knowns.\n",
    "    \"\"\"\n",
    "    titles = list(titles)\n",
    "    length, recommendations = len(movies) + 1,[]\n",
    "    \n",
    "    for i, movie in enumerate(movies):\n",
    "        weight = length - i\n",
    "        dist, index = tree.query([data[titles.index(movie)]], k=3)\n",
    "        for d, m in zip(dist[0], index[0]):\n",
    "            recommendations.append((d*weight, titles[m]))\n",
    "    recommendations.sort()\n",
    "    # Stuff is reorganized by frequency.\n",
    "    rec = [i[1].strip() for i in recommendations if i[1] not in movies]\n",
    "    rec = [i[1] for i in sorted([(v, k) for k, v in Counter(rec).items()],\n",
    "                                reverse=True)]\n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "rec = recommend(movies, tree, titles, data)\n",
    "\n",
    "print('Rank | Movie')\n",
    "print('-----|------')\n",
    "fmt = '{}.   | {}'\n",
    "for index, movie in enumerate(rec[:9]):\n",
    "    print(fmt.format(index + 1, movie))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tadaa!\n",
    "It's not very neat and awesome! But I did like Untraceable to be honest. \n",
    "Some movies are recommended twice! Probably because they are quiet close to multiple choices.\n",
    "\n",
    "## What else can be done?\n",
    "\n",
    "- Feature generation: I've done a nasty job of generating features. That could be cleaned up.\n",
    "- Imputation: A better way of imputing is welcome. Perhaps even need I say.\n",
    "- Some other recommendation method: So far I've only been able to discover KDTrees. If someone could write another one, awesome!\n",
    "\n",
    "*Upvote* to show your appreciation. :D\n",
    "\n",
    "# The final product\n",
    "\n",
    "1. Get movie titles\n",
    "2. Recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = ['hesher', 'leaving las vegas'] # dedicated to A.S.\n",
    "moviecodes, movies = get_movies(names)\n",
    "rec = recommend(movies, tree, titles, data)\n",
    "print('-'*50)\n",
    "print('Recommending on the basis of the above movies')\n",
    "print('-'*50)\n",
    "print()\n",
    "print('+-----|------')\n",
    "print('|Rank | Movie')\n",
    "print('+-----|------')\n",
    "fmt = '|{}.   | {}'\n",
    "for index, movie in enumerate(rec[:9]):\n",
    "    print(fmt.format(index + 1, movie))\n",
    "print('+-----|------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
